{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciBERT Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SciBERT: https://github.com/allenai/scibert\n",
    "- Notebook: https://github.com/huggingface/transformers/blob/master/notebooks/02-transformers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x233b04b4908>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['complementary', 'dual', '-', 'contact', 'switch', 'using', 'soft', 'and', 'hard', 'contact', 'materials', 'for', 'achieving', 'low', 'contact', 'resistance', 'and', 'high', 'reliability', 'simultaneously']\n",
      "Tokens id: [8487, 4793, 579, 3585, 6216, 487, 1720, 137, 2723, 3585, 2518, 168, 9153, 629, 3585, 2661, 137, 597, 4817, 5364]\n",
      "Tokens PyTorch: tensor([[ 102, 8487, 4793,  579, 3585, 6216,  487, 1720,  137, 2723, 3585, 2518,\n",
      "          168, 9153,  629, 3585, 2661,  137,  597, 4817, 5364,  103]])\n",
      "Token wise output: torch.Size([1, 22, 768]), Pooled output: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Tokens comes from a process that splits the input into sub-entities with interesting linguistic properties. \n",
    "tokens = tokenizer.tokenize(\"Complementary Dual-Contact Switch Using Soft and Hard Contact Materials for Achieving Low Contact Resistance and High Reliability Simultaneously\")\n",
    "print(\"Tokens: {}\".format(tokens))\n",
    "\n",
    "# This is not sufficient for the model, as it requires integers as input, \n",
    "# not a problem, let's convert tokens to ids.\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens id: {}\".format(tokens_ids))\n",
    "\n",
    "# Add the required special tokens\n",
    "tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "\n",
    "# We need to convert to a Deep Learning framework specific format, let's use PyTorch for now.\n",
    "tokens_pt = torch.tensor([tokens_ids])\n",
    "print(\"Tokens PyTorch: {}\".format(tokens_pt))\n",
    "\n",
    "# Now we're ready to go through BERT with out input\n",
    "outputs, pooled = model(tokens_pt)\n",
    "print(\"Token wise output: {}, Pooled output: {}\".format(outputs.shape, pooled.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0416, -0.1480, -0.6753,  0.8675,  0.2088,  0.9992,  0.2641, -0.9997,\n",
       "          0.2043,  0.7729,  0.1298,  0.5405,  0.7846,  0.7665, -0.4069, -0.0907,\n",
       "         -0.5304, -0.4133,  0.7156, -0.9106, -0.1457,  0.5916, -0.5221,  0.6518,\n",
       "          0.1237,  0.4443, -0.9847, -0.1341, -0.2130,  0.4250,  0.4813, -0.9459,\n",
       "          0.4612, -0.3520, -0.3998,  0.0332,  0.9989,  0.2931,  0.7156,  0.0493,\n",
       "         -0.5442,  0.2990,  0.4944, -0.4461,  0.5150, -0.0930, -0.4712,  0.3879,\n",
       "         -0.6986,  0.9627,  0.0752, -0.3971, -0.1057,  0.5300,  0.9873,  0.0485,\n",
       "         -0.2699,  0.3184, -0.6361,  0.5775,  0.0217, -0.9774,  0.4412,  0.5084,\n",
       "          0.5062, -0.4235, -0.2550, -0.3205,  0.9495,  0.2289,  0.9662,  0.3846,\n",
       "          0.1071, -0.7250,  0.9554,  0.0102, -0.0994, -0.3838,  0.0567,  0.1527,\n",
       "          0.2297,  0.1733,  0.0352, -0.9719, -0.5161, -0.4337, -0.0113, -0.3069,\n",
       "          0.9672, -0.7773, -0.4541,  0.0539, -0.4561,  0.0442, -0.4799,  0.2286,\n",
       "         -0.5568,  0.1368,  0.1195, -0.3294, -0.4048,  0.0691,  0.3708, -0.4985,\n",
       "         -0.1649, -0.1779, -0.2930, -0.2385, -0.1039,  0.6802, -0.1813, -0.1369,\n",
       "          0.1154, -0.4539, -0.1119,  0.4267,  0.9731,  0.6332, -0.1034,  0.1278,\n",
       "          0.4949, -0.0541,  0.9639, -0.1091, -0.3294, -0.0333, -0.5464,  0.1524,\n",
       "          0.2081,  0.1540, -0.4209,  0.4644, -0.5918, -0.1756, -0.2734,  0.1098,\n",
       "          0.4962,  0.0218,  0.9393, -0.0689, -0.4263, -0.2945, -0.0545,  0.1557,\n",
       "          0.6116,  0.3358,  0.0559,  0.0791,  0.2383,  0.6414, -0.3048, -0.3593,\n",
       "         -0.3908,  0.9976, -0.2609, -0.0796, -0.7665,  0.7430,  0.3581,  0.6486,\n",
       "          0.2229,  0.0159, -0.0761, -0.2760,  0.4320, -0.1492,  0.0761, -0.5120,\n",
       "          0.8507, -0.8693, -0.3426,  0.8054,  0.1354, -0.2426,  0.2911,  0.2636,\n",
       "         -0.8582, -0.2514, -0.9203,  0.9146,  0.2083,  0.9875, -0.1996,  0.1986,\n",
       "          0.1782, -0.8586, -0.3747,  0.4494, -0.5461,  0.1719, -0.5187,  0.4078,\n",
       "          0.2770,  0.3966, -0.9991,  0.1090,  0.4785, -0.3988,  0.1358,  0.1707,\n",
       "          0.3714, -0.9065,  0.1071, -0.6101,  0.7541,  0.3829, -0.5660,  0.0591,\n",
       "         -0.9972,  0.8866,  0.9288, -0.5124, -0.2371,  0.4281,  0.9730,  0.2716,\n",
       "         -0.9241, -0.9968, -0.2195, -0.0925,  0.2699,  0.9655, -0.2254,  0.0411,\n",
       "         -0.6005,  0.6791, -0.5053,  0.0876, -0.4009,  0.3692,  0.0561, -0.9979,\n",
       "         -0.3014,  0.8834, -0.3533,  0.5205,  0.3580,  0.3005, -0.4169, -0.0627,\n",
       "         -0.0773,  0.7489, -0.3394, -0.3506, -0.4260,  0.8877, -0.2933,  0.4190,\n",
       "         -0.6957, -0.9856,  0.6167, -0.5216,  0.8746, -0.3918, -0.0944, -0.5738,\n",
       "          0.8391, -0.9006,  0.7963, -0.0710, -0.1667,  0.4650,  0.0601,  0.1599,\n",
       "         -0.7321,  0.0809, -0.9963,  0.2061, -0.3604, -0.1968, -0.2536, -0.9381,\n",
       "         -0.9645, -0.8747, -0.0432,  0.3281, -0.3339,  0.0628,  0.5285, -0.1166,\n",
       "          0.0701, -0.1787,  0.3410, -0.8978,  0.3191,  0.0069,  0.1266, -0.8641,\n",
       "         -0.3170,  0.1356,  0.9955,  0.8952, -0.3230, -0.4738, -0.5609,  0.4648,\n",
       "         -0.6369,  0.3045, -0.3889,  0.4300, -0.5297, -0.0819,  0.2577,  0.0193,\n",
       "         -0.6426,  0.3830, -0.0189, -0.4553,  0.2643, -0.9923, -0.9317,  0.3354,\n",
       "         -0.4975, -0.6192, -0.4418,  0.1734,  0.1010, -0.1951,  0.9748,  0.4037,\n",
       "         -0.2188, -0.0514,  0.2031,  0.2008,  0.9674,  0.0266,  0.1356,  0.5986,\n",
       "         -0.8045, -0.5842, -0.0462, -0.1507,  0.6958,  0.3096, -0.1518, -0.1764,\n",
       "          0.9968,  0.0196, -0.8750, -0.0466, -0.2000,  0.2783,  0.3600, -0.6096,\n",
       "          0.0511,  0.3521, -0.0552, -0.3122, -0.1607, -0.0810, -0.1337, -0.0114,\n",
       "         -0.9490, -0.3292,  0.3724,  0.1598,  0.2367, -0.6257, -0.2256, -0.1700,\n",
       "         -0.4452, -0.2172,  0.3130,  0.4637, -0.3666,  0.0640, -0.7274,  0.1295,\n",
       "          0.4774, -0.1131,  0.2206, -0.4457, -0.1330,  0.2155, -0.0475, -0.5675,\n",
       "         -0.5051,  0.3830, -0.3525,  0.2788,  0.6116,  0.0182, -0.2822, -0.2569,\n",
       "          0.8846, -0.0311, -0.0935, -0.4207,  0.9911,  0.9795, -0.0238,  0.2702,\n",
       "          0.2881,  0.7691,  0.1559,  0.2860,  0.1005,  0.9993, -0.0155, -0.3393,\n",
       "         -0.9828, -0.4423,  0.2163,  0.0851,  0.1063, -0.0420, -0.4246, -0.7914,\n",
       "         -0.1327,  0.1801,  0.2978,  0.7374,  0.7086, -0.4970,  0.9824,  0.0311,\n",
       "          0.1630, -0.3176, -0.2915,  0.1869,  0.4440, -0.3539, -0.1822,  0.4904,\n",
       "         -0.2101, -0.2411, -0.5285,  0.2968, -0.1934, -0.0348,  0.2126,  0.5274,\n",
       "          0.4042,  0.9998,  0.7292,  0.4610, -0.2071,  0.3394,  0.0724, -0.9370,\n",
       "         -0.0786, -0.2686,  0.8323,  0.7573,  0.1347, -0.0196, -0.5004,  0.5278,\n",
       "         -0.9884, -0.2972, -0.0294, -0.0868,  0.0812,  0.8191,  0.0431, -0.2133,\n",
       "          0.0689, -0.9869, -0.1808, -0.0932, -0.0273,  0.1816,  0.1356,  0.3842,\n",
       "         -0.0688, -0.9765,  0.2089, -0.3097,  0.2525,  0.4863,  0.6546, -0.1553,\n",
       "         -0.7710,  0.3323,  0.9792, -0.1517,  0.9326, -0.0211,  0.1447,  0.4854,\n",
       "         -0.3628, -0.3833,  0.2484, -0.0798, -0.0505, -0.0738, -0.4105,  0.7318,\n",
       "          0.9512,  0.1359, -0.9436, -0.0969, -0.1003, -0.4599, -0.8771,  0.2154,\n",
       "         -0.6469,  0.0592,  0.1454, -0.0163, -0.8268, -0.9899,  0.9329, -0.1708,\n",
       "         -0.1286,  0.9124, -0.9397, -0.3924,  0.9888, -0.2074, -0.9904, -0.0297,\n",
       "          0.8994,  0.1558,  0.4494, -0.2894,  0.5539,  0.6273,  0.3194, -0.1893,\n",
       "         -0.4147, -0.2166,  0.9258,  0.4170, -0.6776, -0.2464, -0.0055,  0.9999,\n",
       "         -0.2654, -0.3309, -0.9810,  0.0260, -0.3424,  0.0073, -0.3945,  0.1532,\n",
       "          0.2895,  0.6775, -0.9672, -0.2593,  0.1869,  0.6343,  0.6534, -0.3145,\n",
       "         -0.2290, -0.1393, -0.9805,  0.9580, -0.2783,  0.3640, -0.0355,  0.4156,\n",
       "         -0.1774, -0.1837, -0.0204, -0.3430, -0.1076,  0.2755, -0.0395,  0.9167,\n",
       "         -0.4678,  0.2974,  0.5708,  0.5639,  0.9986,  0.4741, -0.8286,  0.5910,\n",
       "         -0.2182,  0.2572, -0.4677, -0.9776,  0.2944, -0.6679,  0.3096, -0.4069,\n",
       "          0.2608,  0.2998,  0.5558, -0.0472,  0.8396,  0.9798,  0.9995, -0.4936,\n",
       "          0.3826,  0.7620, -0.1416, -0.4683, -0.0602,  0.4190, -0.0839,  0.9435,\n",
       "         -0.9428, -0.0319, -0.2981,  0.1914,  0.1877,  0.5223, -0.4512, -0.1258,\n",
       "         -0.5326, -0.2117,  0.1205, -0.3300, -0.1290, -0.3234, -0.3544, -0.5649,\n",
       "         -0.1233,  0.4755, -0.6007,  0.0240, -0.0630, -0.3357,  0.3149,  0.6157,\n",
       "         -0.3328, -0.1442,  0.9103, -0.3121,  0.4517, -0.6567, -0.2178, -0.3307,\n",
       "         -0.0034, -0.4409,  0.9827, -0.9391, -0.1273,  0.7315, -0.9991, -0.2666,\n",
       "         -0.2201,  0.3616,  0.1316, -0.9357,  0.2206,  0.3860,  0.5741, -0.9991,\n",
       "          0.1652, -0.9980,  0.1087, -0.1881,  0.3682, -0.8243,  0.6546, -0.0184,\n",
       "         -0.6050,  0.3051, -0.1442,  0.9799,  0.9653, -0.3740, -0.3297, -0.3587,\n",
       "          0.1937,  0.2217,  0.1809, -0.9697,  0.6534,  0.0885,  0.3281,  0.9064,\n",
       "          0.0303,  0.4839, -0.4969,  0.9936, -0.3229, -0.1772, -0.4400,  0.8804,\n",
       "         -0.3160, -0.9825,  0.1065,  0.8626, -0.4526, -0.3301, -0.0557,  0.1420,\n",
       "         -0.3287, -0.9412,  0.3308, -0.3623,  0.1986, -0.3249, -0.5099,  0.3101,\n",
       "          0.5927, -0.9988,  0.1233,  0.2249, -0.1587, -0.5490, -0.8263,  0.3676,\n",
       "         -0.7677, -0.0291, -0.4707, -0.4070,  0.2284,  0.9478,  0.9202,  0.8387,\n",
       "         -0.2694,  0.2556,  0.2134, -0.9097, -0.0358,  0.3654, -0.1768,  0.0137,\n",
       "          0.7435, -0.2173, -0.3221,  0.2255, -0.9699, -0.1819, -0.9804, -0.9970,\n",
       "         -0.5734, -0.0562,  0.9826,  0.2913, -0.7532, -0.9443,  0.2709,  0.3524,\n",
       "         -0.1730,  0.6314, -0.3907,  0.1856,  0.6969,  0.1058,  0.6652,  0.8568,\n",
       "          0.1754, -0.4269,  0.0398, -0.9647,  0.3142,  0.9996, -0.1214,  0.6381,\n",
       "          0.2295,  0.9962, -0.2931,  0.4087, -0.1740,  0.7735, -0.9590,  0.9832,\n",
       "          0.3183, -0.2457,  0.1696,  0.1607, -0.2755,  0.5843, -0.4236, -0.2376,\n",
       "         -0.9963,  0.0428,  0.0577, -0.0914,  0.1700, -0.2190,  0.6924, -0.1045]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\ttensor([[ 102, 8487, 4793,  579, 3585, 6216,  487, 1720,  137, 2723, 3585, 2518,\n",
      "          168, 9153,  629, 3585, 2661,  137,  597, 4817, 5364,  103]])\n",
      "token_type_ids:\n",
      "\ttensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask:\n",
      "\ttensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Difference with previous code: (0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# tokens = tokenizer.tokenize(\"This is an input example\")\n",
    "# tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# tokens_pt = torch.tensor([tokens_ids])\n",
    "\n",
    "# This code can be factored into one-line as follow\n",
    "tokens_pt2 = tokenizer.encode_plus(\"Complementary Dual-Contact Switch Using Soft and Hard Contact Materials for Achieving Low Contact Resistance and High Reliability Simultaneously\", return_tensors=\"pt\")\n",
    "\n",
    "for key, value in tokens_pt2.items():\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))\n",
    "\n",
    "outputs2, pooled2 = model(**tokens_pt2)\n",
    "print(\"Difference with previous code: ({}, {})\".format((outputs2 - outputs).sum(), (pooled2 - pooled).sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
